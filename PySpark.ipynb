{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e95f5c",
   "metadata": {},
   "source": [
    "# Pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16668c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7f9472799310>\n"
     ]
    }
   ],
   "source": [
    "# lambda function\n",
    "f = lambda x : x*x\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5027556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "p = f(10)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1dbabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7f9472799280>\n"
     ]
    }
   ],
   "source": [
    "#Note: We can provide multiple arguments but only one expression\n",
    "f = lambda x,y : x*y\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0535d30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "p = f(10,20)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f854f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostly lambda function used in other functions like filter ,map\n",
    "#filter(): for filering data\n",
    "#1.lambda function   and 2.Iterable type like list \n",
    "x=[10,20,30,40,50]\n",
    "#Task: Filter only those whose value >20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fade9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 40, 50]\n"
     ]
    }
   ],
   "source": [
    "res = list(filter(lambda y: y>20, x))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ece98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 25, 35, 45, 55]\n"
     ]
    }
   ],
   "source": [
    "#map(): for each element Transformation\n",
    "x=[10,20,30,40,50]\n",
    "#map also takes 2 arguments like 1)lambda function 2)Iterable type like list\n",
    "#Task : Add 5 to each element of the list\n",
    "res = list(map(lambda y: y+5 , x))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e17916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35000, 45000, 55000]\n"
     ]
    }
   ],
   "source": [
    "emps=[(\"james\",30000),(\"blake\",40000),(\"rahul\",50000)]\n",
    "#Task: adding 5000 as bonus to each emp salary\n",
    "res = list(map(lambda y : y[1]+5000, emps))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d076c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James', 'Blake', 'Rahul']\n"
     ]
    }
   ],
   "source": [
    "#Task:2 Transform each empname to start with uppercase\n",
    "res2 = list(map(lambda x: x[0].capitalize(), emps))\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d9d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blake', 40000), ('rahul', 50000)]\n"
     ]
    }
   ],
   "source": [
    "#Task :Filter those emps whose sal>30000\n",
    "res3 = list(filter(lambda x : x[1]>30000, emps))\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba324e4",
   "metadata": {},
   "source": [
    "# Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bcdd9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/03 23:18:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e52741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[100, 400, 900, 1600, 2500]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.parallelize([10,20,30,40,50])\n",
    "r2=r1.map(lambda x:x*x)    #here x indicates each element , which is int type\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7ee665c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46ee3b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[10,20,30,40,50]\n",
    "rdd1=sc.parallelize(x)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373c650e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f09deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 400, 900, 1600, 2500]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.parallelize([10,20,30,40,50])\n",
    "r2=r1.map(lambda x:x*x)    #here x indicates each element , which is int type\n",
    "print(r2.collect())\n",
    "type(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea5b956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 25, 35, 45, 55]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r1.map(lambda x:x+5)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1fa9b",
   "metadata": {},
   "source": [
    "# 1.map() :For each element Transformation\n",
    "\n",
    "Applying operation to each element of a RDD and returns a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b85312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 400, 900, 1600, 2500]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.parallelize([10,20,30,40,50])\n",
    "r2=r1.map(lambda x:x*x)    #here x indicates each element , which is int type\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851e247e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 25, 35, 45, 55]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # adding 5 to each element of a RDD\n",
    "r2=r1.map(lambda x:x+5)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba192cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 20, 30], [40, 50, 60]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.flatMap(): flattens the elements \n",
    "x=[[10,20,30],[40,50,60]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec987aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=sc.parallelize(x)\n",
    "z=y.flatMap(lambda x:x)\n",
    "z.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30a73ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2\n",
    "x=[[1,2,3],[4,5,6],[7,8,9],[4,5,6]]\n",
    "rdd1=sc.parallelize(x)\n",
    "rdd2=rdd1.flatMap(lambda x:x)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "617cac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30000, 40000, 50000, 60000]\n"
     ]
    }
   ],
   "source": [
    "# 3.filter :Filters elements of a RDD based on condition and resultant also is a RDD\n",
    "sals=[10000,20000,30000,40000,50000,60000]\n",
    "rdd1=sc.parallelize(sals)\n",
    "rdd2=rdd1.filter(lambda x:x>=30000)\n",
    "res=rdd2.collect()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ba2f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'Devops', 'spark', 'Scala']\n"
     ]
    }
   ],
   "source": [
    "#ex:2\n",
    "a=[\"Python\",\"Java\",\"Devops\",\"spark\",\"Scala\"]\n",
    "rdd1=sc.parallelize(a)\n",
    "rdd2=rdd1.filter(lambda x:x!=\"Java\")\n",
    "res=rdd2.collect()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f200ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 40, 50, 10, 20, 30, 60, 70]\n"
     ]
    }
   ],
   "source": [
    "#4)union(): combines elements of multiple RDDs\n",
    "#by default it performs union all  operation(allows duplicates)\n",
    "r1=sc.parallelize([10,20,30,40,50])\n",
    "r2=sc.parallelize([10,20,30,60,70])\n",
    "res=r1.union(r2)\n",
    "res2=res.collect()\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be34be4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 'Miller', 10000], [102, 'Blake', 20000], [101, 'Miller', 10000], [103, 'James', 30000]]\n"
     ]
    }
   ],
   "source": [
    "r1=sc.parallelize([[101,'Miller',10000],[102,'Blake',20000]])\n",
    "r2=sc.parallelize([[101,'Miller',10000],[103,'James',30000]])\n",
    "r3=r1.union(r2)\n",
    "res=r3.collect()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30b205a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c86e824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f6db2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "404954b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 'Miller', 10000], [102, 'Blake', 20000]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac895150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 'Miller', 10000], [102, 'Blake', 20000], [101, 'Miller', 10000]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95a4ef91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10, 20, 30]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.intersection :return only the common elements\n",
    "r1=sc.parallelize([10,20,30,40,50])\n",
    "r2=sc.parallelize([10,20,30,60,70])\n",
    "r3=r1.intersection(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cb54f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 50]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.subtract():\n",
    "res3=r1.subtract(r2)\n",
    "res3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43e3fc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 70]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re4=r2.subtract(r1)\n",
    "re4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11e04482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11, 10000), (11, 20000), (11, 30000), (12, 10000), (12, 20000), (12, 30000), (13, 10000), (13, 20000), (13, 30000)]\n"
     ]
    }
   ],
   "source": [
    "#7.cartesian() : Each element of left side will merge with each element of right side\n",
    "dnos=sc.parallelize([11,12,13])\n",
    "sals=sc.parallelize([10000,20000,30000])\n",
    "res=dnos.cartesian(sals)\n",
    "res2=res.collect()\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f170825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 40, 10, 30]\n"
     ]
    }
   ],
   "source": [
    "#8.distinct() :Removes the duplicates\n",
    "r1=sc.parallelize([10,20,30,40,10,20,30])\n",
    "res=r1.distinct()\n",
    "res2=res.collect()\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bb8ef",
   "metadata": {},
   "source": [
    "Transformations on a pair RDDs i.e (k,v) pairs\n",
    "\n",
    "1.reduceByKey()\n",
    "2.groupByKey()\n",
    "3.sortByKey()\n",
    "4.mapValues()\n",
    "5.keys()\n",
    "6.values\n",
    "7.join()\n",
    "8.leftOuterJoin()\n",
    "9.rightOuterJoin()\n",
    "10.fullOuterJoin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfa9c6",
   "metadata": {},
   "source": [
    "1.reduceByKey() :sum up all the values with same key\n",
    "                 reduceByKey can be applied only on (k,v) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "282f4fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 70000), (13, 90000), (11, 120000)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,50000),(11,40000),(13,60000),(11,70000)])\n",
    "res=r1.reduceByKey(lambda x,y:x+y)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ecbd61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kohli', 120), ('Rohith', 110), ('Rahul', 90)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.parallelize([(\"Kohli\",40),(\"Rohith\",60),(\"Rahul\",20),(\"Kohli\",80),(\"Rohith\",50),(\"Rahul\",70)])\n",
    "res=r1.reduceByKey(lambda x,y:x+y)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d41e57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c cyclus java-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41d58f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kohli', [120]), ('Rohith', [110]), ('Rahul', [90])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.groupByKey():\n",
    "res2=res.groupByKey().map(lambda x:(x[0],list(x[1])))\n",
    "res2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "634f947f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kohli', 120, 120, 120, 1, 120.0),\n",
       " ('Rohith', 110, 110, 110, 1, 110.0),\n",
       " ('Rahul', 90, 90, 90, 1, 90.0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3=res2.map(lambda x:(x[0],sum(x[1]),max(x[1]),min(x[1]),len(x[1]),sum(x[1])/len(x[1])))\n",
    "res3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e164b4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 10000),\n",
       " (11, 40000),\n",
       " (11, 70000),\n",
       " (12, 20000),\n",
       " (12, 50000),\n",
       " (13, 30000),\n",
       " (13, 60000)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. sortByKey():\n",
    "r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,50000),(11,40000),(13,60000),(11,70000)])\n",
    "res=r1.sortByKey()\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e9331af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 15000),\n",
       " (12, 25000),\n",
       " (13, 35000),\n",
       " (12, 55000),\n",
       " (11, 45000),\n",
       " (13, 65000),\n",
       " (11, 75000)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.mapValues(): Applying a functioanlity to each value,without changing the key\n",
    "r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,50000),(11,40000),(13,60000),(11,70000)])\n",
    "res=r1.mapValues(lambda x:x+5000)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cbe7c809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 15000),\n",
       " (12, 25000),\n",
       " (13, 35000),\n",
       " (12, 55000),\n",
       " (11, 45000),\n",
       " (13, 65000),\n",
       " (11, 75000)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(or) using map()\n",
    "res2=r1.map(lambda x:(x[0],x[1]+5000))\n",
    "res2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "962a8de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 12, 11, 13, 11]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.keys():returns the keys of RDD\n",
    "r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,50000),(11,40000),(13,60000),(11,70000)])\n",
    "res=r1.keys()\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbfecc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10000, 20000, 30000, 50000, 40000, 60000, 70000]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.values():returns the values of a RDD\n",
    "r1=sc.parallelize([(11,10000),(12,20000),(13,30000),(12,50000),(11,40000),(13,60000),(11,70000)])\n",
    "res=r1.values()\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74e67886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, (20, 20)), (30, (40, 40))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. join()\n",
    "r1=sc.parallelize([(10,20),(30,40),(50,60)])\n",
    "r2=sc.parallelize([(10,20),(30,40),(70,80)])\n",
    "# inner join------>only matching tuples\n",
    "\n",
    "ij=r1.join(r2)\n",
    "ij.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2062d140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, (20, 20)), (50, (60, None)), (30, (40, 40))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Left outer Join: Matchings +unmatched of left side\n",
    "loj=r1.leftOuterJoin(r2)\n",
    "loj.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ca62cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, (20, 20)), (30, (40, 40)), (70, (None, 80))]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rigt outer Join: Matchings +unmatched of right side\n",
    "roj=r1.rightOuterJoin(r2)\n",
    "roj.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b9db957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, (20, 20)), (50, (60, None)), (30, (40, 40)), (70, (None, 80))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full outer Join: Matchings +unmatched of left side+unmatched of rightside\n",
    "roj=r1.fullOuterJoin(r2)\n",
    "foj=r1.fullOuterJoin(r2)\n",
    "foj.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be862472",
   "metadata": {},
   "source": [
    "Actions: Whenever action is performed ,the flow executes from it root RDD\n",
    "\n",
    " The following are some of the actions:\n",
    " 1.collect()\n",
    " 2.count()\n",
    " 3.countByKey()\n",
    " 4.countByValue()\n",
    " 5.take(num)\n",
    " 6.top(num)\n",
    " 7.first()\n",
    " 8.reduce()\n",
    " 9.sum()\n",
    "10.max()\n",
    "11.min()\n",
    "12.count()\n",
    "13.saveAsTextFile(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cedfe1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.collect(): It will collect all partitions data of different slave machines into client machine\n",
    "x=[10,20,30,40,50]\n",
    "r1=sc.parallelize(x)\n",
    "print(type(r1))\n",
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d55c24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.count() :counts the no of elements in a RDD\n",
    "r1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0d718b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'IND': 3, 'AUS': 2, 'ENG': 2})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3.countByValue():counts no of times each value occurs in a RDD--->it returns a dictionary\n",
    "medals=[\"IND\",\"AUS\",\"ENG\",\"IND\",\"ENG\",\"AUS\",\"IND\"]\n",
    "r1=sc.parallelize(medals)\n",
    "print(type(r1))\n",
    "r1.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4d59078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'LG': 3, 'Sony': 2})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4,countByKey() :counts the no of times each key had occured\n",
    "#  This should be applied only on a paired RDD\n",
    "pairs=[('LG',10000),('Sony',20000),('Sony',30000),('LG',40000),('LG',50000)]\n",
    "r1=sc.parallelize(pairs)\n",
    "print(type(r1))\n",
    "r1.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24fa6d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "['Ajay', 'Rohit']\n",
      "['Ajay', 'Rohit', 'Amar']\n"
     ]
    }
   ],
   "source": [
    "#5.take(n) :takes first 'n' number of elements of a RDD\n",
    "emps=[\"Ajay\",\"Rohit\",\"Amar\",\"James\",\"John\"]\n",
    "r1=sc.parallelize(emps)\n",
    "print(type(r1))\n",
    "print(r1.take(2))\n",
    "print(r1.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df005310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 20000]\n",
      "[50000, 40000]\n"
     ]
    }
   ],
   "source": [
    "#6.top(n) :Takes top 'n' elements from a RDD\n",
    "sals=[10000,20000,30000,40000,50000]\n",
    "r1=sc.parallelize(sals)\n",
    "print(r1.take(2))\n",
    "print(r1.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4e93af2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ajay', 'Rohit']\n",
      "['Rohit', 'John']\n"
     ]
    }
   ],
   "source": [
    "#ex:2\n",
    "emps=[\"Ajay\",\"Rohit\",\"Amar\",\"James\",\"John\"]\n",
    "r1=sc.parallelize(emps)\n",
    "print(r1.take(2))\n",
    "print(r1.top(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5738953d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ajay'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.first():Takes 1st element of a RDD.\n",
    "r1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3872c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8.reduce(): Combines or sums the elements of a RDD\n",
    "x=[10,20,30,40,50]\n",
    "r1=sc.parallelize(x)\n",
    "print(type(r1))\n",
    "r1.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc2336",
   "metadata": {},
   "source": [
    "#      reduceByKey                               reduce\n",
    "1.groups and agggregates                    1.Cummulative aggregation  \n",
    "\n",
    "2.It is a Transformtion                     2.It is an action\n",
    "\n",
    "3.Applied only on (k,v) pair                3.not on (k,v) pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26ab8474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9.sum() :Finds the sum of RDD eleemnts\n",
    "x=[10,20,30,40,50]\n",
    "r1=sc.parallelize(x)\n",
    "print(type(r1))\n",
    "r1.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5bdb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "50\n",
      "10\n",
      "5\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "#10. sum() ,max(),min(),count()\n",
    "print(r1.sum())\n",
    "print(r1.max())\n",
    "print(r1.min())\n",
    "print(r1.count())\n",
    "print(r1.sum()/r1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da4dccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.saveAsTextFile(path): saving the o/p of a RDD as text File into specified path of HDFS\n",
    "#r1.saveAsTextFile(\"rishabh.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e819e78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increasing the no of partitions while parallelizing\n",
    "x=[10,20,30,40,50,60,70,80]\n",
    "r1=sc.parallelize(x)\n",
    "print(type(x))\n",
    "r1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b4ac7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I Want to increase the partitions while parallelizing\n",
    "r2=sc.parallelize(x,2)\n",
    "r2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd3532b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2.saveAsTextFile('res2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8840a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7d327",
   "metadata": {},
   "source": [
    "# but i doesnt want 2 partitions,make it into one partition\n",
    "\n",
    "for making into onepartition, we have coalesce() function ,which is a transformation function\n",
    "\n",
    "Coalesce(): /it is a transformation to decrease  or combine the partitions of a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fcf86a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d491772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3=r2.coalesce(1)\n",
    "r3.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "944fe1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c8c24845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##can i increase the partitiions using coalesce---??\n",
    "r4=r2.coalesce(3)\n",
    "r4.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e6e1a",
   "metadata": {},
   "source": [
    "#using coalesce, we can only decrease but we cannot increase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b62d9",
   "metadata": {},
   "source": [
    "# Different ways of creating RDDS\n",
    "\n",
    "1.whenever we load a hdfsfile/LFSfile using sparkcontext then RDD is created\n",
    "\n",
    "2.when we perform Transformation on a RDD----->then the resultant is also a RDD\n",
    "\n",
    "3.If we parallelize a Python object---->Then RDD is created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579dadb3",
   "metadata": {},
   "source": [
    "Increasing te no of partitions:2 ways\n",
    "1.while loading the file,we need to specify the no of partitions we want\n",
    "2.while parallelizing,we need to specify the no of partitions we want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4816ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=sc.textFile(\"Book1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98513242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d8b9b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To increase the no of partitions then\n",
    "r1=sc.textFile(\"Book1.txt\",3)\n",
    "r1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f5b6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[10,20,30]\n",
    "y=sc.parallelize(x)\n",
    "print(type(x))\n",
    "y.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2d38959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=sc.parallelize(x,3)\n",
    "y.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcdba1",
   "metadata": {},
   "source": [
    "# Loading data and filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f921e719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4763457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=rdd1.map(lambda x:x.split(\",\"))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "887fc4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Miller', 40000),\n",
       " ('Blake', 50000),\n",
       " ('Sony', 60000),\n",
       " ('Sita', 70000),\n",
       " ('John', 80000)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract only names and salaries\n",
    "rdd3=rdd2.map(lambda x:(x[1],int(x[2])))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "681aeb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sony', 60000), ('Sita', 70000), ('John', 80000)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Task: Filter those employees whose sal>50000\n",
    "rdd4=rdd3.filter(lambda x:x[1]>50000)\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0518ade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ex:2 Getting records of a particular department(11)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cbcd2022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6=rdd2.filter(lambda x:x[4]=='11')\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49526748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spark is simple', 'Spark is for processing', 'spark is distributed']\n"
     ]
    }
   ],
   "source": [
    "#To count the no of occurences of each word in a file\n",
    "lines=[\"spark is simple\",\"Spark is for processing\",\"spark is distributed\"]\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90addd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['spark', 'is', 'simple'],\n",
       " ['Spark', 'is', 'for', 'processing'],\n",
       " ['spark', 'is', 'distributed']]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines1=sc.parallelize(lines)\n",
    "print(type(lines1))\n",
    "words=lines1.map(lambda x:x.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8bf6a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'simple',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'for',\n",
       " 'processing',\n",
       " 'spark',\n",
       " 'is',\n",
       " 'distributed']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1=words.flatMap(lambda x:x)\n",
    "words1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b098e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('is', 1),\n",
       " ('simple', 1),\n",
       " ('Spark', 1),\n",
       " ('is', 1),\n",
       " ('for', 1),\n",
       " ('processing', 1),\n",
       " ('spark', 1),\n",
       " ('is', 1),\n",
       " ('distributed', 1)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To apply reduceByKey,we require (k,v) pair\n",
    "#add 1 to each word to form a key,value pair\n",
    "pair=words1.map(lambda word:(word,1))\n",
    "pair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87bf85a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 2),\n",
       " ('processing', 1),\n",
       " ('is', 3),\n",
       " ('simple', 1),\n",
       " ('Spark', 1),\n",
       " ('for', 1),\n",
       " ('distributed', 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=pair.reduceByKey(lambda x,y:x+y)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b9b36",
   "metadata": {},
   "source": [
    "# Task :select dno,sum(sal) from emp group by dno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d971770",
   "metadata": {},
   "source": [
    "o/p:  11 totsal\n",
    "      12 totsal\n",
    "      13 totsal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7db5b1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e8496a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2: splitting each record---->list\n",
    "rdd2 = rdd1.map(lambda x : x.split(','))\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9a4ea993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('m', 40000), ('m', 50000), ('f', 60000), ('f', 70000), ('m', 80000)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd2.map(lambda x:(x[3],int(x[2])))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45956d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('11', 40000), ('12', 50000), ('11', 60000), ('12', 70000), ('13', 80000)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd2.map(lambda x:(x[4],int(x[2])))\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "feaea45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12', 120000), ('11', 100000), ('13', 80000)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=rdd3.reduceByKey(lambda x,y:x+y)\n",
    "res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dacf67b",
   "metadata": {},
   "source": [
    "# filters:\n",
    "-------\n",
    "filtering nulls or blankspaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49512599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Saprk', 'is', 'for', 'processing', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "line=\"              Saprk is for processing               \"\n",
    "words=line.split(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cf40e630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saprk', 'is', 'for', 'processing']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1=sc.parallelize(words)\n",
    "words2=words1.filter(lambda x:x!='')\n",
    "words2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eb247eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saprk', 'is', 'for', 'processing']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=words2.collect()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d8183aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saprk is for processing\n"
     ]
    }
   ],
   "source": [
    "z=\" \".join(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "250fa765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.shell import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "169cfd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same task using functions\n",
    "# y=eliminatenulls(line)\n",
    "# y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1e0e9c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S a p r k   i s   f o r   p r o c e s s i n g \n"
     ]
    }
   ],
   "source": [
    "str =\"\"\n",
    "for p in z:\n",
    "  str=str+p+\" \"\n",
    " \n",
    "print(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd12392",
   "metadata": {},
   "source": [
    "Groupings and Aggregations:\n",
    "\n",
    "1)Single grouping and Single Aggrgation\n",
    "\n",
    "  dno,sum(sal)\n",
    "2)Multi Grouping and Single Aggregation\n",
    "\n",
    "3)single Grouping and multiple Agggregation\n",
    "\n",
    "4)Multi Grouping amd multiple Aggregation\n",
    "\n",
    "\n",
    "2)Multi Grouping and Single Aggregation\n",
    "\n",
    "ex:select dno,sex,sum(sal) from emp group by dno,sex\n",
    "\n",
    "11\n",
    "   m---totsal\n",
    "   f---totsal\n",
    "12\n",
    "   m---totsal\n",
    "   f---totsal\n",
    "13\n",
    "   m---totsal\n",
    "   f---totsal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "efab0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "def makepair(x):\n",
    "    words=x.split(\",\")\n",
    "    dno=words[4]\n",
    "    sal=int(words[2])\n",
    "    return (dno,sal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af7e414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fdc265bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair=makepair(\"101,Ajay,70000,m,11\")\n",
    "# pair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bcd96467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makepair1(x):\n",
    "   words=x.split(\",\")\n",
    "   dno=int(words[4])\n",
    "   sex=words[3]\n",
    "   sal=int(words[2])\n",
    "   pair=((dno,sex),sal)\n",
    "   return pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3f62729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((11, 'm'), 40000),\n",
       " ((12, 'm'), 50000),\n",
       " ((11, 'f'), 60000),\n",
       " ((12, 'f'), 70000),\n",
       " ((13, 'm'), 80000)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnosexsalpair=r1.map(lambda x:makepair1(x))\n",
    "dnosexsalpair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8da33f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((11, 'm'), 40000),\n",
       " ((11, 'f'), 60000),\n",
       " ((13, 'm'), 80000),\n",
       " ((12, 'm'), 50000),\n",
       " ((12, 'f'), 70000)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum=dnosexsalpair.reduceByKey(lambda x,y:x+y)\n",
    "sum.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "25378f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences b/w reduceByKey() and groupByKey()\n",
    "\n",
    "#       reduceByKey()                            groupByKey\n",
    "\n",
    "# 1.Performance is good when applied on    1.Performance is bad when applied on a single aggregation\n",
    "#   a single aggregation\n",
    "\n",
    "# 2.Bad when applied on multiple aggregation 2.Performance is good when applied on multiple aggregations\n",
    "\n",
    "# 3.Less load on aggregations               3.More load on aggregations\n",
    "\n",
    "# 4.If the no of keys are more then         4.Here no n/w traffic.\n",
    "#   no of partitions will be more\n",
    "#   so more n/w traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1bb74c24",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (452529668.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/452529668.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    # sexsalpair is a RDD divided into 2 partitions\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# Process of groupByKey()\n",
    "# ex:\n",
    "sexsalpair=emparr.map(lambda x:(x[3],int(x[2]))\n",
    "# sexsalpair is a RDD divided into 2 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "939a8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "422d069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex:1 Creating DataFrames from lists (local objects)\n",
    "x=[('Miller',25)]\n",
    "df= sqlContext.createDataFrame(x)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "46a966f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex:1 Creating DataFrames from lists (local objects)\n",
    "x=[('Miller',25)]\n",
    "rdd = sc.parallelize(x)\n",
    "df= rdd.toDF()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7e3fbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7f8a9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d4153db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  NAME|AGE|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex:2 Creating DF with schema\n",
    "x=[('Miller',25),('Ajay',30),('Blake',45)]\n",
    "df=sqlContext.createDataFrame(x,['NAME','AGE'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d5a57ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  NAME|AGE|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex:2 Creating DF with schema\n",
    "x=[('Miller',25),('Ajay',30),('Blake',45)]\n",
    "rdd = sc.parallelize(x)\n",
    "df=rdd.toDF(['NAME','AGE'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02a48011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3)df.count() :To count the no of rows in a DF\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "86009954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- AGE: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4)printSchema():\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "79f5bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25| Ajay|\n",
      "| 30|Rahul|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5) Using Dictionary Providing schema,Creating DF\n",
    "\n",
    "x=[{'name':'Ajay','age':25},{'name':'Rahul','age':30}]\n",
    "df=sqlContext.createDataFrame(x)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fab876ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25| Ajay|\n",
      "| 30|Rahul|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5) Using Dictionary Providing schema,Creating DF\n",
    "\n",
    "x=[{'name':'Ajay','age':25},{'name':'Rahul','age':30}]\n",
    "rdd = sc.parallelize(x)\n",
    "df=rdd.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "205c1a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n",
      "|age| name|city|\n",
      "+---+-----+----+\n",
      "| 25| Ajay|null|\n",
      "| 30|Rahul|Pune|\n",
      "+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex:2\n",
    "\n",
    "x=[{'name':'Ajay','age':25},{'name':'Rahul','age':30,'city':'Pune'}]\n",
    "df=sqlContext.createDataFrame(x)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1c91f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25| Ajay|\n",
      "| 30|Rahul|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ex:2\n",
    "\n",
    "x=[{'name':'Ajay','age':25},{'name':'Rahul','age':30,'city':'Pune'}]\n",
    "rdd = sc.parallelize(x)\n",
    "df=rdd.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f8072603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6) Creating a DF from a RDD\n",
    "\n",
    "x=[('Miller',25),('Ajay',30),('Blake',45)]\n",
    "rdd=sc.parallelize(x)\n",
    "df1=rdd.toDF()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1817973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6) Creating a DF from a RDD\n",
    "\n",
    "x=[('Miller',25),('Ajay',30),('Blake',45)]\n",
    "rdd=sc.parallelize(x)\n",
    "df1=sqlContext.createDataFrame(rdd)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0740204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adding schema to a RDD\n",
    "\n",
    "df2=sqlContext.createDataFrame(rdd,['Name','Age'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1cf3610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n",
      "[Row(Name='Miller', Age=25), Row(Name='Ajay', Age=30), Row(Name='Blake', Age=45)]\n",
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7) Creating a DF using Row object\n",
    "\n",
    "x=[('Miller',25),('Ajay',30),('Blake',45)]\n",
    "rdd1=sc.parallelize(x)\n",
    "#Now providing schema to a RDD\n",
    "from pyspark.sql import Row\n",
    "customer=Row('Name','Age')\n",
    "print(type(customer))\n",
    "cust=rdd1.map(lambda p:customer(*p))\n",
    "print(cust.collect())\n",
    "df3=sqlContext.createDataFrame(cust)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "42aa1e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Miller| 25|\n",
      "|  Ajay| 30|\n",
      "| Blake| 45|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8)Providing schema using StructType\n",
    "from pyspark.sql.types import *\n",
    "schema=StructType([\n",
    "StructField(\"Name\",StringType(),True),\n",
    "StructField(\"Age\",IntegerType(),True)])\n",
    "df4=sqlContext.createDataFrame(rdd1,schema)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f055bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|Empname|EmpAge|\n",
      "+-------+------+\n",
      "| Miller|    25|\n",
      "|   Ajay|    30|\n",
      "|  Blake|    45|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9)changing schema or column names\n",
    "\n",
    "df5=sqlContext.createDataFrame(rdd1,\"Empname:string,EmpAge:int\")\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ec35002d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10) \n",
    "df5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "377288f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(eid=101, ename='miller', sal=10000, sex='m', dno=11), Row(eid=102, ename='Blake', sal=20000, sex='m', dno=12), Row(eid=103, ename='Sony', sal=30000, sex='f', dno=13), Row(eid=104, ename='Sita', sal=40000, sex='f', dno=11), Row(eid=105, ename='James', sal=50000, sex='m', dno=12)]\n",
      "+---+------+-----+---+---+\n",
      "|eid| ename|  sal|sex|dno|\n",
      "+---+------+-----+---+---+\n",
      "|101|miller|10000|  m| 11|\n",
      "|102| Blake|20000|  m| 12|\n",
      "|103|  Sony|30000|  f| 13|\n",
      "|104|  Sita|40000|  f| 11|\n",
      "|105| James|50000|  m| 12|\n",
      "+---+------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#11) Creating emp records\n",
    "\n",
    "from pyspark.sql import Row\n",
    "x=[(101,'miller',10000,'m',11),(102,'Blake',20000,'m',12),(103,'Sony',30000,'f',13),(104,'Sita',40000,'f',11),(105,'James',50000,'m',12)]\n",
    "r1=sc.parallelize(x)\n",
    "df=sqlContext.createDataFrame(r1,['eid','ename','sal','sex','dno'])\n",
    "print(df.collect())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "427a01f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(eid=101, ename='miller', sal=10000, sex='m', dno=11), Row(eid=102, ename='Blake', sal=20000, sex='m', dno=12), Row(eid=103, ename='Sony', sal=30000, sex='f', dno=13), Row(eid=104, ename='Sita', sal=40000, sex='f', dno=11), Row(eid=105, ename='James', sal=50000, sex='m', dno=12)]\n",
      "+---+------+-----+---+---+\n",
      "|eid| ename|  sal|sex|dno|\n",
      "+---+------+-----+---+---+\n",
      "|101|miller|10000|  m| 11|\n",
      "|102| Blake|20000|  m| 12|\n",
      "|103|  Sony|30000|  f| 13|\n",
      "|104|  Sita|40000|  f| 11|\n",
      "|105| James|50000|  m| 12|\n",
      "+---+------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#11) Creating emp records\n",
    "\n",
    "from pyspark.sql import Row\n",
    "x=[(101,'miller',10000,'m',11),(102,'Blake',20000,'m',12),(103,'Sony',30000,'f',13),(104,'Sita',40000,'f',11),(105,'James',50000,'m',12)]\n",
    "r1=sc.parallelize(x)\n",
    "df=r1.toDF(['eid','ename','sal','sex','dno'])\n",
    "print(df.collect())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cfec0e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e74e5429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "132bbc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\",1)\n",
    "# r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f416997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ff04ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r1.map(lambda x:x.split(\",\"))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3746b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#each element to be converted to row type to create an DF\n",
    "r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4]),city=x[5]))\n",
    "df=sqlContext.createDataFrame(r3)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b21a5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#each element to be converted to row type to create an DF\n",
    "r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4]),city=x[5]))\n",
    "df=r3.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ffa05",
   "metadata": {},
   "source": [
    "# Different DataFrame API's:\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f38ff245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "73c6582d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r1.map(lambda x:x.split(\",\"))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "df24f48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11, city='hyd'),\n",
       " Row(eid=102, ename='Blake', sal=50000, sex='m', dno=12, city='pune'),\n",
       " Row(eid=103, ename='Sony', sal=60000, sex='f', dno=11, city='pune'),\n",
       " Row(eid=104, ename='Sita', sal=70000, sex='f', dno=12, city='hyd'),\n",
       " Row(eid=105, ename='John', sal=80000, sex='m', dno=13, city='hyd')]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4]),city=x[5]))\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b2603dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.createDataFrame(r3)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3f159185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| ename|\n",
      "+------+\n",
      "|Miller|\n",
      "| Blake|\n",
      "|  Sony|\n",
      "|  Sita|\n",
      "|  John|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Different API's of DF\n",
    "\n",
    "#1)select() :to select (or) extract a particular column\n",
    "\n",
    "e1=df.select(\"ename\")\n",
    "e1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde534eb",
   "metadata": {},
   "source": [
    "Transformation on a DF---->returns a DF\n",
    "here df---->has 6 fields\n",
    "e1 is a dataframe--->with one field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bff9b9",
   "metadata": {},
   "source": [
    "previously to extract a particular field from a RDD or to create a (k,v)paired RDD->using index\n",
    "which is complex \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ccf29c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| ename|  sal|\n",
      "+------+-----+\n",
      "|Miller|40000|\n",
      "| Blake|50000|\n",
      "|  Sony|60000|\n",
      "|  Sita|70000|\n",
      "|  John|80000|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To extract multiple columns\n",
    "df.select(\"ename\",\"sal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d99557d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations\n",
    "#   Adding 3000 to each employee salary\n",
    "# df2=df.select(\"ename\",\"sal\"+\"3000\")\n",
    "# here we get error bcoz--->whenever we perform arithmetic operations or with expressions,\n",
    "# we need to use the dataframe seperately to each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d4a4619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "| ename|(sal + 3000)|\n",
      "+------+------------+\n",
      "|Miller|       43000|\n",
      "| Blake|       53000|\n",
      "|  Sony|       63000|\n",
      "|  Sita|       73000|\n",
      "|  John|       83000|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.select(df.ename,df.sal+3000)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3d186001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "| ename|(sal + 3000)|\n",
      "+------+------------+\n",
      "|Miller|       43000|\n",
      "| Blake|       53000|\n",
      "|  Sony|       63000|\n",
      "|  Sita|       73000|\n",
      "|  John|       83000|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectExpr():\n",
    "df.selectExpr(\"ename\",\"sal+3000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "66454311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---+---+----+\n",
      "|eid|ename|  sal|sex|dno|city|\n",
      "+---+-----+-----+---+---+----+\n",
      "|104| Sita|70000|  f| 12| hyd|\n",
      "|105| John|80000|  m| 13| hyd|\n",
      "+---+-----+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter():\n",
    "\n",
    "# Filter only those emps whose sal>60000\n",
    "\n",
    "df3=df.filter(df.sal>60000)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a02e7cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|ename|  sal|\n",
      "+-----+-----+\n",
      "| Sita|70000|\n",
      "| John|80000|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df.filter(df.sal>60000).select(\"ename\",\"sal\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f36ede76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| ename|  sal|\n",
      "+------+-----+\n",
      "|Miller|40000|\n",
      "|  Sony|60000|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4=df.filter(df.dno==11).select(\"ename\",\"sal\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "56bef7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11, city='hyd'),\n",
       " Row(eid=102, ename='Blake', sal=50000, sex='m', dno=12, city='pune'),\n",
       " Row(eid=103, ename='Sony', sal=60000, sex='f', dno=11, city='pune'),\n",
       " Row(eid=104, ename='Sita', sal=70000, sex='f', dno=12, city='hyd'),\n",
       " Row(eid=105, ename='John', sal=80000, sex='m', dno=13, city='hyd')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6)collect()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d32f4052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count(): Returns the no of rows in a DataFrame\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5844339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eid', 'ename', 'sal', 'sex', 'dno', 'city']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns: Return column names as list\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "597cd96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eid: long (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- sal: long (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- dno: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printSchema():\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c34a97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 188:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+------------------+----+------------------+----+\n",
      "|summary|               eid|ename|               sal| sex|               dno|city|\n",
      "+-------+------------------+-----+------------------+----+------------------+----+\n",
      "|  count|                 5|    5|                 5|   5|                 5|   5|\n",
      "|   mean|             103.0| null|           60000.0|null|              11.8|null|\n",
      "| stddev|1.5811388300841898| null|15811.388300841896|null|0.8366600265340753|null|\n",
      "|    min|               101|Blake|             40000|   f|                11| hyd|\n",
      "|    max|               105| Sony|             80000|   m|                13|pune|\n",
      "+-------+------------------+-----+------------------+----+------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#describe()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5b19156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|dno|\n",
      "+---+\n",
      "| 11|\n",
      "| 12|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #performs diff aggregations on numerical columns\n",
    "# distinct(): returns a new DF containing distinct rows\n",
    "\n",
    "# I want to know the no of deptnos\n",
    "# i.e I want to know the distinct dnos\n",
    "df6=df.select(\"dno\")\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "01f19846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|dno|\n",
      "+---+\n",
      "| 12|\n",
      "| 11|\n",
      "| 13|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1a66f402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|dno|\n",
      "+---+\n",
      "| 12|\n",
      "| 11|\n",
      "| 13|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(or) \n",
    "df.select(\"dno\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bd2572ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+----+\n",
      "|eid| ename|  sal|dno|city|\n",
      "+---+------+-----+---+----+\n",
      "|101|Miller|40000| 11| hyd|\n",
      "|102| Blake|50000| 12|pune|\n",
      "|103|  Sony|60000| 11|pune|\n",
      "|104|  Sita|70000| 12| hyd|\n",
      "|105|  John|80000| 13| hyd|\n",
      "+---+------+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop() :drops a particular column\n",
    "df7=df.drop(\"sex\")\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3234bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----+\n",
      "|eid| ename|  sal|city|\n",
      "+---+------+-----+----+\n",
      "|101|Miller|40000| hyd|\n",
      "|102| Blake|50000|pune|\n",
      "|103|  Sony|60000|pune|\n",
      "|104|  Sita|70000| hyd|\n",
      "|105|  John|80000| hyd|\n",
      "+---+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we can also drop multiple columns at a time\n",
    "df8=df.drop(\"sex\",\"dno\")\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d873e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropDuplicates(): drops duplicates based on multiple columns\n",
    "df9=df.dropDuplicates([\"dno\",\"sex\"])\n",
    "#here 11,m is repeated for 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "39a32a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "15f0dcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11, city='hyd')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first(): reurns the first row in the dataframe\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5cb72e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach() :\n",
    "# ex: Applying a function foreach row of a df\n",
    "def display(df):\n",
    "    print(df.ename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3c8fd5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sita\n",
      "John\n",
      "Miller\n",
      "Blake\n",
      "Sony\n"
     ]
    }
   ],
   "source": [
    "df.foreach(display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "59932fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort():\n",
    "df.sort(df.sal).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9cd456f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.sal.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ee87c07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#orderBy():\n",
    "df.orderBy(\"ename\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "52f84900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1b0f822a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repartition():\n",
    "df.repartition(5).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0e1f8863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#repartition on specified column :It will shuffle the data\n",
    "df2=df.repartition(\"dno\",\"sex\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8a26f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2415560a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+------+---+----+\n",
      "|eid| ename|  sal|   sex|dno|city|\n",
      "+---+------+-----+------+---+----+\n",
      "|101|Miller|40000|  male| 11| hyd|\n",
      "|102| Blake|50000|  male| 12|pune|\n",
      "|103|  Sony|60000|female| 11|pune|\n",
      "|104|  Sita|70000|female| 12| hyd|\n",
      "|105|  John|80000|  male| 13| hyd|\n",
      "+---+------+-----+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace():\n",
    "df2=df.replace(['m','f'],['male','female'],'sex')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2ae6a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---+---+----+\n",
      "|eid|ename|  sal|sex|dno|city|\n",
      "+---+-----+-----+---+---+----+\n",
      "|101| Ajay|40000|  m| 11| hyd|\n",
      "|102|Blake|50000|  m| 12|pune|\n",
      "|103| Sony|60000|  f| 11|pune|\n",
      "|104| Sita|70000|  f| 12| hyd|\n",
      "|105|James|80000|  m| 13| hyd|\n",
      "+---+-----+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.replace(['Miller','John'],['Ajay','James'],'ename')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f84abc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+---+----+\n",
      "|ecode| ename|income|Gender|dno|city|\n",
      "+-----+------+------+------+---+----+\n",
      "|  101|Miller| 40000|     m| 11| hyd|\n",
      "|  102| Blake| 50000|     m| 12|pune|\n",
      "|  103|  Sony| 60000|     f| 11|pune|\n",
      "|  104|  Sita| 70000|     f| 12| hyd|\n",
      "|  105|  John| 80000|     m| 13| hyd|\n",
      "+-----+------+------+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 21)to change schema or the column names\n",
    "\n",
    "# eid---->ecode\n",
    "# sal---->income\n",
    "# sex---->Gender\n",
    "df2=df.toDF('ecode','ename','income','Gender','dno','city')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "78c05798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+---+---+----+\n",
      "|eid|empname|  sal|sex|dno|city|\n",
      "+---+-------+-----+---+---+----+\n",
      "|101| Miller|40000|  m| 11| hyd|\n",
      "|102|  Blake|50000|  m| 12|pune|\n",
      "|103|   Sony|60000|  f| 11|pune|\n",
      "|104|   Sita|70000|  f| 12| hyd|\n",
      "|105|   John|80000|  m| 13| hyd|\n",
      "+---+-------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#withColumnRenamed(existing,new):Renaming a particular column\n",
    "df.withColumnRenamed(\"ename\",\"empname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2069e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+-----+\n",
      "|eid| ename|  sal|sex|dno|city|  tax|\n",
      "+---+------+-----+---+---+----+-----+\n",
      "|101|Miller|40000|  m| 11| hyd|35000|\n",
      "|102| Blake|50000|  m| 12|pune|45000|\n",
      "|103|  Sony|60000|  f| 11|pune|55000|\n",
      "|104|  Sita|70000|  f| 12| hyd|65000|\n",
      "|105|  John|80000|  m| 13| hyd|75000|\n",
      "+---+------+-----+---+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#withColumn(): Adding a new column\n",
    "df.withColumn('tax',df.sal-5000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4db39df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"eid\":101,\"ename\":\"Miller\",\"sal\":40000,\"sex\":\"m\",\"dno\":11,\"city\":\"hyd\"}',\n",
       " '{\"eid\":102,\"ename\":\"Blake\",\"sal\":50000,\"sex\":\"m\",\"dno\":12,\"city\":\"pune\"}',\n",
       " '{\"eid\":103,\"ename\":\"Sony\",\"sal\":60000,\"sex\":\"f\",\"dno\":11,\"city\":\"pune\"}',\n",
       " '{\"eid\":104,\"ename\":\"Sita\",\"sal\":70000,\"sex\":\"f\",\"dno\":12,\"city\":\"hyd\"}',\n",
       " '{\"eid\":105,\"ename\":\"John\",\"sal\":80000,\"sex\":\"m\",\"dno\":13,\"city\":\"hyd\"}']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toJSON()\n",
    "\n",
    "df.toJSON().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "96aa21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11, city='hyd'), Row(eid=102, ename='Blake', sal=50000, sex='m', dno=12, city='pune'), Row(eid=103, ename='Sony', sal=60000, sex='f', dno=11, city='pune'), Row(eid=104, ename='Sita', sal=70000, sex='f', dno=12, city='hyd'), Row(eid=105, ename='John', sal=80000, sex='m', dno=13, city='hyd')]\n"
     ]
    }
   ],
   "source": [
    "#toLocalIterator():returns local python iterator object such as list,tuple,set,dict\n",
    "l1=list(df.toLocalIterator())\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1fcd1ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|sex|count|\n",
      "+---+-----+\n",
      "|  m|    3|\n",
      "|  f|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy():\n",
    "\n",
    "# #Task: select sex,count(*) from emp group by sex\n",
    "res1=df.groupBy(\"sex\").count()\n",
    "res1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7bbe1b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|sex|count|\n",
      "+---+-----+\n",
      "|  m|    3|\n",
      "|  f|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 27) Aggregated functions:\n",
    "#   i)agg()\n",
    "#  II)sum()\n",
    "#  iii)max()\n",
    "#   iv)min()\n",
    "#    v)avg()\n",
    "#   vi)count()\n",
    "\n",
    "# Case 1: Single grouping and single aggregation\n",
    "\n",
    "# o/p:\n",
    "# m---->count\n",
    "# f---->count\n",
    "df.groupBy(\"sex\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6d69d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|sex|sum(sal)|\n",
      "+---+--------+\n",
      "|  m|  170000|\n",
      "|  f|  130000|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ii)sum aggregation:\n",
    "# o/p:\n",
    "# m----->totsal\n",
    "# f----->totsal\n",
    "df.groupBy(\"sex\").sum(\"sal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4d5b396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|sex|          avg(sal)|\n",
      "+---+------------------+\n",
      "|  m|56666.666666666664|\n",
      "|  f|           65000.0|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#iii)avg\n",
    "df.groupBy(\"sex\").avg(\"sal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d7869357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|dno|max(sal)|\n",
      "+---+--------+\n",
      "| 12|   70000|\n",
      "| 11|   60000|\n",
      "| 13|   80000|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#iv)max\n",
    "df.groupBy(\"dno\").max(\"sal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b8949930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|dno|min(sal)|\n",
      "+---+--------+\n",
      "| 12|   50000|\n",
      "| 11|   40000|\n",
      "| 13|   80000|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#V)min\n",
    "df.groupBy(\"dno\").min(\"sal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "73a1f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "|dno|sex|sum(sal)|\n",
      "+---+---+--------+\n",
      "| 12|  m|   50000|\n",
      "| 11|  f|   60000|\n",
      "| 11|  m|   40000|\n",
      "| 12|  f|   70000|\n",
      "| 13|  m|   80000|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#28)Multigroupings:\n",
    "\n",
    "res1=df.groupBy(\"dno\",\"sex\").sum(\"sal\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "525fa04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 29)union(): merging the rows of 2DFs\n",
    "\n",
    "# The Merging DF should have the same schema\n",
    "\n",
    "# #syntax:  df1.union(df2)\n",
    "df.union(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "689d32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.union(df2).union(df3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "aeb0c490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+----+\n",
      "|eid| ename|  sal|sex|dno|city|\n",
      "+---+------+-----+---+---+----+\n",
      "|102| Blake|50000|  m| 12|pune|\n",
      "|104|  Sita|70000|  f| 12| hyd|\n",
      "|105|  John|80000|  m| 13| hyd|\n",
      "|103|  Sony|60000|  f| 11|pune|\n",
      "|101|Miller|40000|  m| 11| hyd|\n",
      "+---+------+-----+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 30.intersect(): The common rows will be returned\n",
    "\n",
    "# The DFs should have the same schema\n",
    "\n",
    "df.intersect(df).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c74e2fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11, city='hyd'),\n",
       " Row(eid=102, ename='Blake', sal=50000, sex='m', dno=12, city='pune'),\n",
       " Row(eid=103, ename='Sony', sal=60000, sex='f', dno=11, city='pune'),\n",
       " Row(eid=104, ename='Sita', sal=70000, sex='f', dno=12, city='hyd'),\n",
       " Row(eid=105, ename='John', sal=80000, sex='m', dno=13, city='hyd')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#31.Converting DF to a RDD\n",
    "rdd1=df.rdd\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6015e00",
   "metadata": {},
   "source": [
    "32.joins:\n",
    "\n",
    "joins: used to collect data from two or more datasets\n",
    "\n",
    "Horizontal merging: merging cols horizontally\n",
    "ex:joins\n",
    "\n",
    "There are 2 types of JOINS\n",
    "1)Inner join\n",
    "2)Outer Join---------->3types--->i)Left outer Join\n",
    "                                ii)Rigt outer Join\n",
    "                               iii)Full outer Join\n",
    "\n",
    "\n",
    "ex:1\n",
    "A=1 B=1\n",
    "  2   2\n",
    "  3   3\n",
    "  4   7\n",
    "  5   8\n",
    "  6   9\n",
    "\n",
    "1)Inner Join :Only Matching fields\n",
    "\n",
    "o/p:\n",
    "(1,1)\n",
    "(2,2)\n",
    "(3,3)\n",
    "\n",
    "2)Left Outer Join: Matchings+unmatched fields of leftside i.e Total presence\n",
    "                              of left side\n",
    "\n",
    "(1,1)\n",
    "(2,2)\n",
    "(3,3)\n",
    "(4, )\n",
    "(5, )\n",
    "(6, )\n",
    "3)Right Outer Join: Matchings+unmatched fields of rightside i.e Total presence\n",
    "                              of right side\n",
    "\n",
    "(1,1)\n",
    "(2,2)\n",
    "(3,3)\n",
    "( ,7)\n",
    "( ,8)\n",
    "( ,9)\n",
    "\n",
    "4)Full Outer Join: Matchings+unmatched fields of leftside i.e Total presence\n",
    "                              of left side\n",
    "                            +unmatched fields of rightside i.e Total presence\n",
    "                              of right side\n",
    "\n",
    "\n",
    "\n",
    "                                                                    \n",
    "                               \n",
    "\n",
    "(1,1)\n",
    "(2,2)\n",
    "(3,3)\n",
    "(4, )\n",
    "(5, )\n",
    "(6, )\n",
    "( ,7)\n",
    "( ,8)\n",
    "( ,9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb434f42",
   "metadata": {},
   "source": [
    "# Working with SQL querries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4f020",
   "metadata": {},
   "source": [
    "working with sql queries:\n",
    "-------------------------\n",
    "for working with sql queries,Register DataFrame as table and apply all valid sql queries on it\n",
    "\n",
    "\n",
    "To Register DF as table-->we have 2 ways\n",
    "\n",
    "1.df.registerTempTable(\"tablename\")\n",
    "\n",
    "2.sqlContext.registerDataFrameAsTable(df,\"tablename\")\n",
    "\n",
    "  sqlContext.sql(\".......sql query......\").show()\n",
    "\n",
    "ex:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "66c08cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "49526256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r1.map(lambda x:x.split(\",\"))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "660f9b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11),\n",
       " Row(eid=102, ename='Blake', sal=50000, sex='m', dno=12),\n",
       " Row(eid=103, ename='Sony', sal=60000, sex='f', dno=11),\n",
       " Row(eid=104, ename='Sita', sal=70000, sex='f', dno=12),\n",
       " Row(eid=105, ename='John', sal=80000, sex='m', dno=13)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "74724b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+\n",
      "|eid| ename|  sal|sex|dno|\n",
      "+---+------+-----+---+---+\n",
      "|101|Miller|40000|  m| 11|\n",
      "|102| Blake|50000|  m| 12|\n",
      "|103|  Sony|60000|  f| 11|\n",
      "|104|  Sita|70000|  f| 12|\n",
      "|105|  John|80000|  m| 13|\n",
      "+---+------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emps_df=sqlContext.createDataFrame(r3)\n",
    "emps_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8682c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+\n",
      "|eid| ename|  sal|sex|dno|\n",
      "+---+------+-----+---+---+\n",
      "|101|Miller|40000|  m| 11|\n",
      "|102| Blake|50000|  m| 12|\n",
      "|103|  Sony|60000|  f| 11|\n",
      "|104|  Sita|70000|  f| 12|\n",
      "|105|  John|80000|  m| 13|\n",
      "+---+------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #I-way:\n",
    "emps_df.registerTempTable(\"emps1\")\n",
    "e1=sqlContext.sql(\"select * from emps1\")\n",
    "e1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0477cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+\n",
      "|eid| ename|  sal|sex|dno|\n",
      "+---+------+-----+---+---+\n",
      "|101|Miller|40000|  m| 11|\n",
      "|102| Blake|50000|  m| 12|\n",
      "|103|  Sony|60000|  f| 11|\n",
      "|104|  Sita|70000|  f| 12|\n",
      "|105|  John|80000|  m| 13|\n",
      "+---+------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#II-way:\n",
    "sqlContext.registerDataFrameAsTable(emps_df,\"emps2\")\n",
    "e2=sqlContext.sql(\"select * from emps2\")\n",
    "e2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d079b40",
   "metadata": {},
   "source": [
    "Here emps2 is the tablename ,on this table,we can perform any valid sql queries'\n",
    "\n",
    "HDFSfile---->RDD----->DF---->table---->sqlquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2cf04850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+-------+\n",
      "|eid| ename|  sal|sex|dno|    tax|\n",
      "+---+------+-----+---+---+-------+\n",
      "|101|Miller|40000|  m| 11|4000.00|\n",
      "|102| Blake|50000|  m| 12|5000.00|\n",
      "|103|  Sony|60000|  f| 11|6000.00|\n",
      "|104|  Sita|70000|  f| 12|7000.00|\n",
      "|105|  John|80000|  m| 13|8000.00|\n",
      "+---+------+-----+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding extra column\n",
    "\n",
    "tax_add_df=sqlContext.sql(\"select *,sal*0.10 as tax from emps2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "469df1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|sex|sum(sal)|\n",
      "+---+--------+\n",
      "|  m|  170000|\n",
      "|  f|  130000|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#case 1: Single grouping and single Aggregation\n",
    "\n",
    "singlegrp_aggr=sqlContext.sql(\"select sex,sum(sal) from emps2 group by sex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "acb11058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+\n",
      "|dno|sex|sum(sal)|\n",
      "+---+---+--------+\n",
      "| 12|  m|   50000|\n",
      "| 11|  f|   60000|\n",
      "| 11|  m|   40000|\n",
      "| 12|  f|   70000|\n",
      "| 13|  m|   80000|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#case 2: Multigrouping and single Aggregation\n",
    "\n",
    "#dnowise,sexwise----->sum(sal)\n",
    "\n",
    "multigrp=sqlContext.sql(\"select dno,sex,sum(sal) from emps2 group by dno,sex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c90e3943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+--------+--------+--------+\n",
      "|sex|sum(sal)|          avg(sal)|max(sal)|min(sal)|count(1)|\n",
      "+---+--------+------------------+--------+--------+--------+\n",
      "|  m|  170000|56666.666666666664|   80000|   40000|       3|\n",
      "|  f|  130000|           65000.0|   70000|   60000|       2|\n",
      "+---+--------+------------------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Case 3: Single grouping and multiple aggregation\n",
    "\n",
    "multiaggr=sqlContext.sql(\"select sex,sum(sal),avg(sal),max(sal),min(sal),count(*) from emps2 group by sex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1b7fa49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+--------+--------+--------+--------+\n",
      "|dno|sex|sum(sal)|avg(sal)|max(sal)|min(sal)|count(1)|\n",
      "+---+---+--------+--------+--------+--------+--------+\n",
      "| 12|  m|   50000| 50000.0|   50000|   50000|       1|\n",
      "| 11|  f|   60000| 60000.0|   60000|   60000|       1|\n",
      "| 11|  m|   40000| 40000.0|   40000|   40000|       1|\n",
      "| 12|  f|   70000| 70000.0|   70000|   70000|       1|\n",
      "| 13|  m|   80000| 80000.0|   80000|   80000|       1|\n",
      "+---+---+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Case 4: Multigrouping and multiple aggregation:\n",
    "\n",
    "multigrp_aggr=sqlContext.sql(\"select dno,sex,sum(sal),avg(sal),max(sal),min(sal),count(*) from emps2 group by dno,sex\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769b9a5",
   "metadata": {},
   "source": [
    "joins:\n",
    "\n",
    "Task: Citywise------>I want the total sal budget\n",
    "\n",
    "hadoop@ubuntu:~$ cat > emps1\n",
    "101,aaa,1000,m,11\n",
    "102,bbb,2000,f,12\n",
    "103,ccc,3000,m,12\n",
    "104,ddd,4000,f,13\n",
    "105,eee,5000,m,11\n",
    "106,fff,6000,f,14\n",
    "107,ggg,7000,m,15\n",
    "108,hhh,8000,f,16hadoop@ubuntu:~$ \n",
    "\n",
    "hadoop@ubuntu:~$ cat >dept1\n",
    "11,mrkt,hyd\n",
    "12,HR,delhi\n",
    "13,fin,pune\n",
    "17,HR,hyd\n",
    "18,fin,pune\n",
    "19,mrkt,delhihadoop@ubuntu:~$ hdfs dfs -put emps1 /pysparklab\n",
    "put: Cannot create file/pysparklab/emps1._COPYING_. Name node is in safe mode.\n",
    "hadoop@ubuntu:~$ hdfs dfsadmin -safemode leave\n",
    "Safe mode is OFF\n",
    "hadoop@ubuntu:~$ hdfs dfs -put emps1 /pysparklab\n",
    "hadoop@ubuntu:~$ hdfs dfs -put dept1 /pysparklab\n",
    "\n",
    "Task: Citywise------>I want the total sal budget\n",
    "\n",
    "step 1: Loading both emp and dept from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6b67390a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['101,Miller,40000,m,11,hyd',\n",
       " '102,Blake,50000,m,12,pune',\n",
       " '103,Sony,60000,f,11,pune',\n",
       " '104,Sita,70000,f,12,hyd',\n",
       " '105,John,80000,m,13,hyd']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 =sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book1.txt\")\n",
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f930f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['101', 'Miller', '40000', 'm', '11', 'hyd'],\n",
       " ['102', 'Blake', '50000', 'm', '12', 'pune'],\n",
       " ['103', 'Sony', '60000', 'f', '11', 'pune'],\n",
       " ['104', 'Sita', '70000', 'f', '12', 'hyd'],\n",
       " ['105', 'John', '80000', 'm', '13', 'hyd']]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=r1.map(lambda x:x.split(\",\"))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f2e6d8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(eid=101, ename='Miller', sal=40000, sex='m', dno=11),\n",
       " Row(eid=102, ename='Blake', sal=50000, sex='m', dno=12),\n",
       " Row(eid=103, ename='Sony', sal=60000, sex='f', dno=11),\n",
       " Row(eid=104, ename='Sita', sal=70000, sex='f', dno=12),\n",
       " Row(eid=105, ename='John', sal=80000, sex='m', dno=13)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "r3=r2.map(lambda x:Row(eid=int(x[0]),ename=x[1],sal=int(x[2]),sex=x[3],dno=int(x[4])))\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5b4ec18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+---+---+\n",
      "|eid| ename|  sal|sex|dno|\n",
      "+---+------+-----+---+---+\n",
      "|101|Miller|40000|  m| 11|\n",
      "|102| Blake|50000|  m| 12|\n",
      "|103|  Sony|60000|  f| 11|\n",
      "|104|  Sita|70000|  f| 12|\n",
      "|105|  John|80000|  m| 13|\n",
      "+---+------+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emps1=sqlContext.createDataFrame(r3)\n",
    "emps1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6909040c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city,dname,dno',\n",
       " 'hyd,mrkt,11',\n",
       " 'delhi,HR,12',\n",
       " 'pune,fin,13',\n",
       " 'hyd,HR,17',\n",
       " 'delhi,fin,18',\n",
       " 'pune,mrkt,19']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now loading dept into HDFS\n",
    "rr1=sc.textFile(\"/Users/rishabhshukla/Desktop/Pyspark_codes/PySpark/Book12.txt\")\n",
    "rr1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "29d39d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['city', 'dname', 'dno'],\n",
       " ['hyd', 'mrkt', '11'],\n",
       " ['delhi', 'HR', '12'],\n",
       " ['pune', 'fin', '13'],\n",
       " ['hyd', 'HR', '17'],\n",
       " ['delhi', 'fin', '18'],\n",
       " ['pune', 'mrkt', '19']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr2=rr1.map(lambda x:x.split(\",\"))\n",
    "rr2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "03ad7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rr3=rr2.map(lambda x:Row(city=x[0],dname=x[1],dno=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7b5e8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c cyclus java-jdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3caaffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d2a5295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  conda update conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9b708cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b5a6ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install nb_conda_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "de092d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "print(findspark.init())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f1f17b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/03 23:20:08 ERROR Executor: Exception in task 0.0 in stage 292.0 (TID 681)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'dno'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1255/1731462402.apply(Unknown Source)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "22/10/03 23:20:08 WARN TaskSetManager: Lost task 0.0 in stage 292.0 (TID 681) (192.168.15.65 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'dno'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1255/1731462402.apply(Unknown Source)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "22/10/03 23:20:08 ERROR TaskSetManager: Task 0 in stage 292.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 292.0 failed 1 times, most recent failure: Lost task 0.0 in stage 292.0 (TID 681) (192.168.15.65 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDD$$Lambda$1255/1731462402.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4290/1846325275.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4288/298701419.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDD$$Lambda$866/1911106641.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDD$$Lambda$1255/1731462402.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2615089761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrr3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 292.0 failed 1 times, most recent failure: Lost task 0.0 in stage 292.0 (TID 681) (192.168.15.65 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDD$$Lambda$1255/1731462402.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4290/1846325275.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4288/298701419.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDD$$Lambda$866/1911106641.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDD$$Lambda$1255/1731462402.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rr3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e9045272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/03 23:20:09 ERROR Executor: Exception in task 0.0 in stage 293.0 (TID 683)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'dno'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$$Lambda$1487/581454672.apply(Unknown Source)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "22/10/03 23:20:09 WARN TaskSetManager: Lost task 0.0 in stage 293.0 (TID 683) (192.168.15.65 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'dno'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD$$$Lambda$1487/581454672.apply(Unknown Source)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "22/10/03 23:20:09 ERROR TaskSetManager: Task 0 in stage 293.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 293.0 failed 1 times, most recent failure: Lost task 0.0 in stage 293.0 (TID 683) (192.168.15.65 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD$$$Lambda$1487/581454672.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4290/1846325275.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4288/298701419.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD$$$Lambda$1487/581454672.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/1273276912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdept1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdept1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \"\"\"\n\u001b[0;32m--> 471\u001b[0;31m         return self.sparkSession.createDataFrame(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \"\"\"\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \"\"\"\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The first row in RDD is empty, \"\u001b[0m \u001b[0;34m\"can not infer schema\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1901\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m         \"\"\"\n\u001b[0;32m-> 1903\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1904\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 293.0 failed 1 times, most recent failure: Lost task 0.0 in stage 293.0 (TID 683) (192.168.15.65 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD$$$Lambda$1487/581454672.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4290/1846325275.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$4288/298701419.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/Users/rishabhshukla/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0y/g5w79j0j11b9ftwrx642wkm00000gn/T/ipykernel_1741/2010100705.py\", line 2, in <lambda>\nValueError: invalid literal for int() with base 10: 'dno'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD$$$Lambda$1487/581454672.apply(Unknown Source)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext$$Lambda$1257/59185492.apply(Unknown Source)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1174/1568411563.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "dept1=sqlContext.createDataFrame(rr3)\n",
    "dept1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b90eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b7401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
